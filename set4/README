1.1 Most of the data we need to run a BFS should not be stored in global memory,
as this incurs unnecessary transactions that slow down the program's execution
time. Because j is an arbitrary integer (less than the number of vertices), the
writes to C and F incurred by the code within the for loop, as well as the reads
from X, would require the entire lists to be loaded into shared memory for each
individual block, requiring a number of transactions which quickly outpaces that
of having each thread on the frontier accessing global memory (except for in
special cases where the frontier is most of the graph, or there is a small
number of blocks). Additionally, if the writes to C and F were stored in shared
memory first, each block would have to go through all of C and F and atomically
change the values in global memory, while they could just do this for only the
necessary values by taking care of it during the for loop. There's also no
reason to copy Va to shared memory, since having each thread access two
consecutive values makes for coalesced transactions, and only reads the number
of values needed, rather than the entire list. However, since it is possible to
figure out exactly which elements of Ea a block will need by getting everything
in between the first value needed by the first thread and the last value needed
by the last thread, there is some efficiency to be gained by storing this in
shared memory. While it is unlikely that the warps reading these values in will
fall on a single cache line and this all but guarantees bank conflicts as 32
threads try to access 32 values that may be stored in any bank, it is still
better than the penalty of requiring potentially a transaction with global
memory per thread per iteration of the loop.

1.2 A simple parallelizable way to check whether F is not all false at each
iteration is to store the true/false values as 1 or 0 and use a kernel to
compute the list's sum (which we know how to do), as it will only be 0 if every
element is 0.

1.3 One other way to check whether F is not all false is to keep a boolean
or integer value (varying between 0 and 1) in global memory which is set to
false right before the kernel is called, and is atomically updated to true
whenever a thread sets F[j] to true. The major disadvantage of this method is
the fact that while the method of summing up the frontier array is solely
dependent on how many vertices the graph has (i.e. the size of the array), this
method suffers a dramatic performance decrease on dense graphs, since many
threads may look at the same F[j] if the frontier nodes that they're processing
are connected to it, and thus the value of the "all false" variable may end up
being checked more times than the number of computations required to compute
the sum. On sparse graphs, however, the potential number of times it will need
to be checked is small at any level, whereas computing the sum of F may still be
quite intensive if the set of vertices is large.


2.1 The performance of GPU-accelerated PET reconstruction will almost certainly
be worse than that of X-ray CT reconstruction, as the data and method of
reconstruction are not nearly as suited to parallelization. The most significant
disadvantage of PET reconstruction will likely be the fact that every single
addition operation must be atomic, since many detected emissions may correspond
to the same pixel, and for certain detector locations this can mean up to
sqrt(width^2 + height^2) atomic operations if the line the detector corresponds
to is the resulting image's diagonal. Furthermore, there is no efficient way to
cache the detection data in texture memory as with the CT scans, as it is just
one long unordered list, and thus there is no better way to read it than as a
single element per thread, from global memory. Moving it to local memory doesn't
make sense, either, since reading one element per thread is already coalesced,
and each data point only needs to be read once, so we're stuck with using global
memory.